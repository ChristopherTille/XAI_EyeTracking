{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import skdim\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import keras\n",
    "import cv2\n",
    "import skdim\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "#from google.colab.patches import cv2_imshow\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from matplotlib import pyplot as plt\n",
    "from skopt import gp_minimize\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from itertools import combinations\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import Lasso, lars_path, Ridge, ElasticNet, LogisticRegression, SGDClassifier\n",
    "from collections import Counter\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "from scipy.stats import mannwhitneyu\n",
    "from scipy.stats import f_oneway\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import wilcoxon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data and convert to data and label\n",
    "def import_data(folder_path, train):\n",
    "  \"\"\"\n",
    "  input:\n",
    "    dataset is whether you want to get the training or test data\n",
    "    folder_path is the link to the file containing the image data (e.g.\"XXX\")\n",
    "    target_size is the desired size after resizing the images (e.g. 224*224 pixels)\n",
    "  output:\n",
    "    images_array is the array containing the pixel values of the images with shape (8, 224, 224, 3) and value range in [0,255]\n",
    "    str_label_array is the array containing the string labels of the images\n",
    "    int_label_array is the array containing the integer labels of the images\n",
    "  \"\"\"\n",
    "\n",
    "  if train==1:\n",
    "    folder_path = os.path.join(folder_path, \"Train 2800\")\n",
    "    print('Importing Training data...')\n",
    "  elif train == 2:\n",
    "    print('Importing Explanation data...')\n",
    "    folder_path = os.path.join(folder_path, \"Pool 25\")\n",
    "  elif train == 0:\n",
    "    print('Importing Test data...')\n",
    "    folder_path = os.path.join(folder_path, \"Test 700\")  \n",
    "\n",
    "  class_folders = [f for f in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, f))]\n",
    "\n",
    "  # Initialize an empty list to store pixel values\n",
    "  images = []\n",
    "  ground_truth_labels = []\n",
    "  image_names = []\n",
    "\n",
    "  for class_folder in class_folders:\n",
    "        class_folder_path = os.path.join(folder_path, class_folder)\n",
    "        image_files = [f for f in os.listdir(class_folder_path) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "\n",
    "        for image_file in image_files:\n",
    "            image_path = os.path.join(class_folder_path, image_file)\n",
    "            image_names.append(image_file)\n",
    "            img = cv2.imread(image_path)\n",
    "            #resized_img = cv2.resize(img, target_size)\n",
    "            images.append(img)\n",
    "            ground_truth_labels.append(class_folder)\n",
    "\n",
    "  images_array = np.array(images)\n",
    "  print('Imported', images_array.shape[0], 'images of shape', images_array.shape[1:4])\n",
    "\n",
    "\n",
    "  str_ground_truth_labels = np.array(ground_truth_labels)\n",
    "\n",
    "  label_mapping = {\"Tel-Aviv\": \"TelAviv\",\n",
    "          \"West Jerusalem\": \"Jerusalem\",\n",
    "          \"WestJerusalem\": \"Jerusalem\",\n",
    "          \"Hamburg\": \"Hamburg\",\n",
    "          \"Berlin\": \"Berlin\"}\n",
    "\n",
    "  # Map original class labels to new label names\n",
    "  str_ground_truth_labels = np.array([label_mapping[label] for label in str_ground_truth_labels])\n",
    "  print('Remapped to the following classes: ', np.unique(str_ground_truth_labels, return_counts=True)[0])\n",
    "  print('Found', np.unique(str_ground_truth_labels, return_counts=True)[1], 'examples for the different classes respectively')\n",
    "\n",
    "  # Assuming you have a function strLabel_to_intLabel_mapping that converts string labels to integers\n",
    "  int_ground_truth_labels = strLabel_to_intLabel_mapping(str_ground_truth_labels)\n",
    "\n",
    "  cat_ground_truth_labels = to_categorical(int_ground_truth_labels, 4)\n",
    "\n",
    "\n",
    "  return images_array, int_ground_truth_labels, cat_ground_truth_labels, str_ground_truth_labels, image_names\n",
    "\n",
    "# Map string labels (e.g. \"Jerusalem\") to integer labels (e.g. 1)\n",
    "def strLabel_to_intLabel_mapping(y):\n",
    "  \"\"\"\n",
    "  input:\n",
    "    y is the array of string labels\n",
    "  output:\n",
    "    int_labels_mapped is the array of the corresponding integer labels\n",
    "  \"\"\"''\n",
    "  # Create a dictionary to map string labels to int labels\n",
    "  label_mapping = {'TelAviv': 2, 'Jerusalem': 3, 'Berlin': 0, 'Hamburg': 1}\n",
    "  # Map string labels to int labels using the created dictionary\n",
    "  int_labels_mapped = np.array([label_mapping[val] for val in y])\n",
    "  return int_labels_mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_int_train, y_cat_train, y_str_train, image_names_train = import_data(\"XXX\", train = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(X_train[0], cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_int_test, y_cat_test, y_str_test, image_names_test = import_data(\"XXX\", train = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(X_test[0], cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_expl, y_int_expl, y_cat_expl, y_str_expl, image_names_expl = import_data(\"XXX\", train = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "for i in range(16):\n",
    "    ax = plt.subplot(4, 4, i + 1)\n",
    "    i = random.randint(0,99)\n",
    "    plt.imshow(cv2.cvtColor(X_expl[i], cv2.COLOR_BGR2RGB))\n",
    "    plt.title(y_str_expl[i])\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "plt.suptitle('Random examples of study images', fontsize=13)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('studyimages.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (448, 448)\n",
    "IMG_SHAPE = IMG_SIZE + (3,)\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE, include_top=False, weights='imagenet')\n",
    "\n",
    "base_model.trainable = False\n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "prediction_layer = tf.keras.layers.Dense(4, activation='softmax')\n",
    "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "\n",
    "inputs = tf.keras.Input(shape=(448, 448, 3))\n",
    "x = preprocess_input(inputs)\n",
    "x = base_model(x, training=False)\n",
    "x = global_average_layer(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "outputs = prediction_layer(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "base_learning_rate = 0.0001\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('fine_train_on_array.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = np.argmax(model.predict(X_test), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_int_test, pred_test)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "class_report = classification_report(y_int_test, pred_test)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_int_test, pred_test)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_expl = np.argmax(model.predict(X_expl), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_int_expl, pred_expl)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "class_report = classification_report(y_int_expl, pred_expl)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_int_expl, pred_expl)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (448, 448)\n",
    "IMG_SHAPE = IMG_SIZE + (3,)\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE, include_top=False, weights='imagenet')\n",
    "\n",
    "base_model.trainable = False\n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "prediction_layer = tf.keras.layers.Dense(4, activation='softmax')\n",
    "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "\n",
    "inputs = tf.keras.Input(shape=(448, 448, 3))\n",
    "x = preprocess_input(inputs)\n",
    "x = base_model(x, training=False)\n",
    "\n",
    "grad_model = tf.keras.Model(inputs, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_model.load_weights('fine_train_on_array.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weights = model.get_weights()\n",
    "\n",
    "for i, weight in enumerate(weights):\n",
    "    print(f\"Layer {i} weights shape: {weight.shape}\")\n",
    "    print(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "weights = grad_model.get_weights()\n",
    "\n",
    "for i, weight in enumerate(weights):\n",
    "    print(f\"Layer {i} weights shape: {weight.shape}\")\n",
    "    print(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_names = [layer.name for layer in model.layers]\n",
    "classifier_layer_names = layer_names[-3:]\n",
    "print(classifier_layer_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradCAM(img_array, model, grad_conv_model, classifier_layer_names):\n",
    "    \n",
    "    classifier_input = tf.keras.Input(shape=grad_conv_model.output.shape[1:])\n",
    "    x = classifier_input\n",
    "    for layer_name in classifier_layer_names:\n",
    "        x = model.get_layer(layer_name)(x)\n",
    "    classifier_model = keras.Model(classifier_input, x)   \n",
    "      \n",
    "    with tf.GradientTape() as tape:\n",
    "        last_conv_layer_output = grad_conv_model(img_array)\n",
    "        tape.watch(last_conv_layer_output)\n",
    "        preds = classifier_model(last_conv_layer_output)\n",
    "        top_pred_index = tf.argmax(preds[0])\n",
    "        top_class_channel = preds[:, top_pred_index]\n",
    "    grads = tape.gradient(top_class_channel, last_conv_layer_output) \n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "    \n",
    "    last_conv_layer_output = last_conv_layer_output.numpy()[0]\n",
    "    pooled_grads = pooled_grads.numpy()\n",
    "    for i in range(pooled_grads.shape[-1]):\n",
    "        last_conv_layer_output[:, :, i] *= pooled_grads[i]\n",
    "        \n",
    "    heatmap = np.mean(last_conv_layer_output, axis=-1)\n",
    "    heatmap = np.maximum(heatmap, 0)\n",
    "    heatmap /= np.max(heatmap)\n",
    "    \n",
    "    \n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradCAMplusplus(img_array, model, grad_conv_model, classifier_layer_names):\n",
    "    classifier_input = tf.keras.Input(shape=grad_conv_model.output.shape[1:])\n",
    "    x = classifier_input\n",
    "    for layer_name in classifier_layer_names:\n",
    "        x = model.get_layer(layer_name)(x)\n",
    "    classifier_model = keras.Model(classifier_input, x)\n",
    "        \n",
    "    with tf.GradientTape() as tape1:\n",
    "        with tf.GradientTape() as tape2:\n",
    "            with tf.GradientTape() as tape3:\n",
    "                last_conv_layer_output = grad_conv_model(img_array)\n",
    "                #tape.watch(last_conv_layer_output)\n",
    "                preds = classifier_model(last_conv_layer_output)\n",
    "                \n",
    "                top_pred_index = tf.argmax(preds[0])\n",
    "                top_class_channel = preds[:, top_pred_index]\n",
    "                \n",
    "                conv_first_grad = tape3.gradient(top_class_channel, last_conv_layer_output)\n",
    "            conv_second_grad = tape2.gradient(conv_first_grad, last_conv_layer_output)\n",
    "        conv_third_grad = tape1.gradient(conv_second_grad, last_conv_layer_output)\n",
    "    \n",
    "    global_sum = np.sum(last_conv_layer_output, axis=(0, 1, 2))\n",
    "    \n",
    "    alpha_num = conv_second_grad[0]\n",
    "    alpha_denom = conv_second_grad[0]*2.0 + conv_third_grad[0]*global_sum\n",
    "    alpha_denom = np.where(alpha_denom != 0.0, alpha_denom, 1e-10)\n",
    "    \n",
    "    alphas = alpha_num/alpha_denom\n",
    "    alpha_normalization_constant = np.sum(alphas, axis=(0,1))\n",
    "    alphas /= alpha_normalization_constant\n",
    "    \n",
    "    weights = np.maximum(conv_first_grad[0], 0.0)\n",
    "    \n",
    "    deep_linearization_weights = np.sum(weights*alphas, axis=(0,1))\n",
    "    grad_cam_map = np.sum(deep_linearization_weights*last_conv_layer_output[0], axis=2)\n",
    "\n",
    "    heatmap = np.maximum(grad_cam_map, 0)\n",
    "    max_heat = np.max(heatmap)\n",
    "    \n",
    "    if max_heat == 0:\n",
    "        max_heat = 1e-10\n",
    "    heatmap /= max_heat\n",
    "    \n",
    "    \n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample_gradCAM(activations):\n",
    "    heatmap = cv2.resize(activations, (448, 448))\n",
    "    heatmap /= np.sum(heatmap)\n",
    "\n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_gradCAM(activations, img_index):\n",
    "    heatmap = cv2.resize(activations, (448, 448))\n",
    "    heatmap = np.expand_dims(heatmap, axis=-1)\n",
    "    heatmap /= np.sum(heatmap)\n",
    "    explanation = heatmap*X_expl[img_index]\n",
    "    explanation -=np.min(explanation)\n",
    "    explanation /= np.max(explanation)\n",
    "    explanation = cv2.cvtColor(explanation, cv2.COLOR_BGR2RGB)\n",
    "    #plt.matshow(explanation)\n",
    "    #plt.show()  \n",
    "    \n",
    "    return explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_gradCAM(activations, img_index, alpha):\n",
    "    heatmap = cv2.resize(activations, (448, 448))\n",
    "    heatmap = (heatmap*255).astype(\"uint8\")\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "    superimposed_img = heatmap * alpha + X_expl[img_index]\n",
    "    superimposed_img = np.clip(superimposed_img, 0, 255).astype(\"uint8\")\n",
    "    superimposed_img = cv2.cvtColor(superimposed_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    #plt.matshow(superimposed_img)\n",
    "    #plt.show()  \n",
    "    \n",
    "    return superimposed_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_human_attention(activations, img_index, alpha):\n",
    "    #heatmap = cv2.resize(activations, (448, 448))\n",
    "    heatmap = activations*(1/np.max(activations))\n",
    "    heatmap = (heatmap*255).astype(\"uint8\")\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "    superimposed_img = heatmap * alpha + X_expl[img_index]\n",
    "    superimposed_img = np.clip(superimposed_img, 0, 255).astype(\"uint8\")\n",
    "    superimposed_img = cv2.cvtColor(superimposed_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    #plt.matshow(superimposed_img)\n",
    "    #plt.show()  \n",
    "    \n",
    "    return superimposed_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_conf_drop(model, img_index, explanation):\n",
    "    y_i = model.predict(np.expand_dims(X_expl[img_index], axis=0), verbose = 0)\n",
    "    pred = y_int_expl[img_index]\n",
    "    y_i_c = y_i[0][pred]\n",
    "    o_i_c = model.predict(np.expand_dims((explanation*255).astype(\"uint8\"), axis=0), verbose = 0)[0][pred]\n",
    "    print('yi:', y_i)\n",
    "    print('oi:', model.predict(np.expand_dims((explanation*255).astype(\"uint8\"), axis=0), verbose = 0)[0])\n",
    "    print('yic:', y_i_c)\n",
    "    print('oic:', o_i_c)\n",
    "    conf_drop = np.maximum(0,y_i_c-o_i_c)/y_i_c\n",
    "    \n",
    "    return conf_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_conf_inc(model, img_index, explanation):\n",
    "    y_i = model.predict(np.expand_dims(X_expl[img_index], axis=0), verbose = 0)\n",
    "    pred = y_int_expl[img_index]\n",
    "    y_i_c = y_i[0][pred]\n",
    "    o_i_c = model.predict(np.expand_dims((explanation*255).astype(\"uint8\"), axis=0), verbose = 0)[0][pred]\n",
    "\n",
    "    conf_inc = y_i_c < o_i_c\n",
    "\n",
    "    return conf_inc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_conf_win(model, img_index, explanation_pp, explanation):\n",
    "    y_i = model.predict(np.expand_dims(X_expl[img_index], axis=0), verbose = 0)\n",
    "    pred = y_int_expl[img_index]\n",
    "    y_i_c = y_i[0][pred]\n",
    "    \n",
    "    o_i_c_pp = model.predict(np.expand_dims((explanation_pp*255).astype(\"uint8\"), axis=0), verbose = 0)[0][pred]\n",
    "    o_i_c = model.predict(np.expand_dims((explanation*255).astype(\"uint8\"), axis=0), verbose = 0)[0][pred]\n",
    "    \n",
    "    conf_drop_pp = np.maximum(0,y_i_c-o_i_c_pp)\n",
    "    conf_drop = np.maximum(0,y_i_c-o_i_c)\n",
    "\n",
    "    return conf_drop_pp<conf_drop, conf_drop_pp>conf_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_map(attention_map):\n",
    "    # Define custom colormap colors\n",
    "    #colors = [(0, 'white'), (0.33, 'lightgreen'), (0.66, 'yellow'), (1, 'red')]\n",
    "    # Create the colormap\n",
    "    #eyetracking_cmap = LinearSegmentedColormap.from_list(\"eyetracking_heatmap\", colors)\n",
    "    # Visualize with the custom colormap\n",
    "    plt.imshow(attention_map)#, cmap=eyetracking_cmap, interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    #plt.title('Eyetracking Heatmap')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create GradCAM ++ and GradCAM activations for each image (14x14) and save locally as *stimulus name*_GradCAMpp.npy or *stimulus name*_GradCAM.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filepath = 'XXX'\n",
    "\n",
    "for i in range(X_expl.shape[0]):\n",
    "    gradCAM_pp_activations = gradCAMplusplus(img_array = np.expand_dims(X_expl[i], axis=0), model = model, grad_conv_model = grad_model, classifier_layer_names= classifier_layer_names)\n",
    "    stimulus_name, extension = os.path.splitext(image_names_expl[i])\n",
    "    filename = filepath+'/'+stimulus_name+'_GradCAMpp.npy'\n",
    "    print(filename)\n",
    "    np.save(filename, gradCAM_pp_activations)\n",
    "    gradCAM_activations = gradCAM(img_array = np.expand_dims(X_expl[i], axis=0), model = model, grad_conv_model = grad_model, classifier_layer_names= classifier_layer_names)\n",
    "    filename = filepath+'/'+stimulus_name+'_GradCAM.npy'\n",
    "    print(filename)\n",
    "    np.save(filename, gradCAM_activations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataframe for each heatmap comparison (human vs. ai) using similarity measures and other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filepath_GradCAM = 'XXX'\n",
    "filepath_Human_Heatmaps = 'XXX'\n",
    "filepath_StudyTimelines = 'XXX'\n",
    "filepath_Umfrage = 'XXX'\n",
    "\n",
    "umfrage = pd.read_excel(filepath_Umfrage, header = 1)\n",
    "umfrage = umfrage.iloc[:,6:-16]\n",
    "umfrage = umfrage.drop(columns=['Teilnehmende Person: [Keine Beschreibung] 04'\n",
    "                                , 'Teilnehmende Person: Ausweichoption (negativ) oder Anzahl ausgewählter Optionen'\n",
    "                                , 'Datenschutz'\n",
    "                                , 'Staatsangehörigkeit: Ausweichoption (negativ) oder Anzahl ausgewählter Optionen'])\n",
    "\n",
    "umfrage['Teilnehmende Person: Student/Studentin'] = umfrage['Teilnehmende Person: Student/Studentin'].replace({'ausgewählt': 1, 'nicht gewählt': 0})\n",
    "umfrage['Teilnehmende Person: Mitarbeiter/Mitarbeiterin'] = umfrage['Teilnehmende Person: Mitarbeiter/Mitarbeiterin'].replace({'ausgewählt': 1, 'nicht gewählt': 0})\n",
    "umfrage['Teilnehmende Person: Extern'] = umfrage['Teilnehmende Person: Extern'].replace({'ausgewählt': 1, 'nicht gewählt': 0})\n",
    "umfrage['Staatsangehörigkeit: Deutsch'] = umfrage['Staatsangehörigkeit: Deutsch'].replace({'ausgewählt': 1, 'nicht gewählt': 0})\n",
    "umfrage['Staatsangehörigkeit: Sonstige'] = umfrage['Staatsangehörigkeit: Sonstige'].replace({'ausgewählt': 1, 'nicht gewählt': 0})\n",
    "umfrage['Berlin'] = umfrage['Berlin'].replace({'Ja': 1, 'Nein': 0})\n",
    "umfrage['Hamburg'] = umfrage['Hamburg'].replace({'Ja': 1, 'Nein': 0})\n",
    "umfrage['Tel Aviv'] = umfrage['Tel Aviv'].replace({'Ja': 1, 'Nein': 0})\n",
    "umfrage['Jerusalem'] = umfrage['Jerusalem'].replace({'Ja': 1, 'Nein': 0})\n",
    "umfrage['Sehhilfe'] = umfrage['Sehhilfe'].replace({'Nein, ich trage weder Brille noch Kontaktlinsen': 0, 'Ja, ich trage Kontaktlinsen': 1, 'Ja, ich trage eine Brille': 2})\n",
    "umfrage['Augenerkrankung'] = umfrage['Augenerkrankung'].replace({'Nein': 0})\n",
    "umfrage['Farbenblindheit'] = umfrage['Farbenblindheit'].replace({'Nein': 0, 'Ja': 1})\n",
    "\n",
    "\n",
    "umfrage = umfrage.rename(columns={'UserID: [01]': 'user id', 'Teilnehmende Person: Student/Studentin': 'student'\n",
    "                                  , 'Teilnehmende Person: Mitarbeiter/Mitarbeiterin': 'employee'\n",
    "                                  , 'Teilnehmende Person: Extern': 'external'\n",
    "                                  , 'Geschlecht': 'sex'\n",
    "                                  , 'Geburtsjahr: [01]': 'year of birth'\n",
    "                                  , 'Staatsangehörigkeit: Deutsch': 'nationality: german'\n",
    "                                  , 'Staatsangehörigkeit: Sonstige': 'nationality: others'\n",
    "                                  , 'Staatsangehörigkeit: Sonstige (offene Eingabe)': 'nationality: input'\n",
    "                                  , 'Fähigkeiten':'expertise', 'Sehhilfe': 'visual aid'\n",
    "                                  , 'Augenerkrankung': 'eye disease'\n",
    "                                  , 'Bildungsabschluss': 'educational qualification'\n",
    "                                  , 'Farbenblindheit': 'color blindness'})\n",
    "\n",
    "columns = ['CID', 'user', 'user id', 'stimuli', 'image index', 'true label', 'human prediction'\n",
    "           , 'human correct', 'ai prediction'\n",
    "           , 'wasserstein distance' , 'pearson correlation', 'mae'\n",
    "           , 'wasserstein distance ++' , 'pearson correlation ++', 'mae ++'\n",
    "           , 'confidence drop', 'confidence increase', 'win'\n",
    "           , 'confidence drop ++', 'confidence increase ++', 'win ++']\n",
    "\n",
    "rows = []\n",
    "CID = 0\n",
    "\n",
    "label_map = {0: 'B', 1: 'H', 2: 'T', 3: 'J'}\n",
    "\n",
    "    \n",
    "for user in os.listdir(filepath_Human_Heatmaps):\n",
    "    print('User: ',user)\n",
    "    user_id = int(user.split()[-1])\n",
    "    study_timeline = pd.read_excel(filepath_StudyTimelines, sheet_name=user)\n",
    "        \n",
    "    for stimuli in os.listdir(os.path.join(filepath_Human_Heatmaps, user)):\n",
    "        if stimuli.endswith('.npy') and stimuli.startswith('img'):\n",
    "            print('****************************************')\n",
    "            stimulus_name, extension = os.path.splitext(stimuli)\n",
    "            print('Stimuli name: ',stimulus_name)\n",
    "            \n",
    "            class_name_of_stimuli = stimulus_name.split('CLASS_')[1][0]\n",
    "            #print('True label: ',class_name_of_stimuli)\n",
    "            \n",
    "            img_index = image_names_expl.index(stimulus_name+'.png')\n",
    "            #print('Stimuli image ID: ', img_index)\n",
    "            \n",
    "            #load user's attention map for one stimuli\n",
    "            human_attention = np.load(os.path.join(filepath_Human_Heatmaps, user, stimuli))\n",
    "            #print('Shape of human attention: ', human_attention.shape)\n",
    "            #print('Mass of human attention: ', np.sum(human_attention))\n",
    "            print('Human attention map:')\n",
    "            print('Mass on attention map: ',np.sum(human_attention))\n",
    "            plot_attention_map(human_attention)\n",
    "            #extract user's decision for this stimuli\n",
    "            human_decision = study_timeline['Antwort (B,H,T,J)'][study_timeline['Bild']==stimulus_name+'.png'].values[0]\n",
    "            #print('Human pred: ', human_decision)\n",
    "            human_correct = study_timeline['Richtig?'][study_timeline['Bild']==stimulus_name+'.png'].values[0]\n",
    "            \n",
    "            #model prediction\n",
    "            ai_decision = pd.Series(np.argmax(model.predict(np.expand_dims(X_expl[img_index], axis=0), verbose = 0)))\n",
    "            #print(ai_decision)\n",
    "            ai_decision = ai_decision.map(label_map).values[0]\n",
    "            #print('AI pred: ', ai_decision)\n",
    "            \n",
    "            #get GradCAM activations\n",
    "            gradCAM_pp_attention = np.load(filepath_GradCAM+'/'+stimulus_name+'_GradCAMpp.npy') \n",
    "            gradCAM_attention = np.load(filepath_GradCAM+'/'+stimulus_name+'_GradCAM.npy')\n",
    "            \n",
    "            #upsample actications to get XAI attention map\n",
    "            gradCAM_pp_up = upsample_gradCAM(gradCAM_pp_attention)\n",
    "            gradCAM_up = upsample_gradCAM(gradCAM_attention)\n",
    "            \n",
    "            print('GradCAM ++ attention map:')\n",
    "            print('Mass on attention map: ',np.sum(gradCAM_pp_up))\n",
    "            plot_attention_map(gradCAM_pp_up)\n",
    "            print('GradCAM attention map:')\n",
    "            print('Mass on attention map: ',np.sum(gradCAM_up))\n",
    "            plot_attention_map(gradCAM_up)  \n",
    "            \n",
    "            #print('Shape of GradCAM ++ attention: ', gradCAM_pp_up.shape)\n",
    "            #print('Mass of GradCAM ++ attention: ', np.sum(gradCAM_pp_up))\n",
    "            #print('Max of GradCAM ++ attention: ', np.max(gradCAM_pp_up.flatten()))\n",
    "            #print('Min of GradCAM ++ attention: ', np.min(gradCAM_pp_up.flatten()))\n",
    "            \n",
    "            #print('Shape of GradCAM attention: ', gradCAM_up.shape)\n",
    "            #print('Mass of GradCAM attention: ', np.sum(gradCAM_up))    \n",
    "            #print('Max of GradCAM attention: ', np.max(gradCAM_up.flatten()))\n",
    "            #print('Min of GradCAM attention: ', np.min(gradCAM_up.flatten()))     \n",
    "            \n",
    "            wasserstein_distance_GradCAM_pp = wasserstein_distance(gradCAM_pp_up.flatten(), human_attention.flatten())\n",
    "            #print(wasserstein_distance_GradCAM_pp)\n",
    "            wasserstein_distance_GradCAM = wasserstein_distance(gradCAM_up.flatten(), human_attention.flatten())\n",
    "            #print(wasserstein_distance_GradCAM)\n",
    "            \n",
    "            pearson_corr_GradCAM_pp = np.corrcoef(gradCAM_pp_up.flatten(), human_attention.flatten())[0, 1]\n",
    "            #print(pearson_corr_GradCAM_pp)\n",
    "            pearson_corr_GradCAM = np.corrcoef(gradCAM_up.flatten(), human_attention.flatten())[0, 1]\n",
    "            #print(pearson_corr_GradCAM)\n",
    "            \n",
    "            MAE_GradCAM_pp = np.mean(np.abs(gradCAM_pp_up.flatten() - human_attention.flatten()))\n",
    "            #print(MAE_GradCAM_pp)\n",
    "            MAE_GradCAM = np.mean(np.abs(gradCAM_up.flatten() - human_attention.flatten()))\n",
    "            #print(MAE_GradCAM)\n",
    "            \n",
    "            #explanation for model input\n",
    "            print('GradCAM ++ explanation map:')\n",
    "            gradCAM_pp_expl = explain_gradCAM(activations = gradCAM_pp_attention, img_index = img_index)\n",
    "            print('GradCAM explanation map:')\n",
    "            gradCAM_expl = explain_gradCAM(activations = gradCAM_attention, img_index = img_index)\n",
    "    \n",
    "            #faithfulness evaluation metrics for each image\n",
    "            conf_drop_pp_i = calculate_conf_drop(model = model, img_index = img_index, explanation = gradCAM_pp_expl)\n",
    "            conf_drop_i = calculate_conf_drop(model = model, img_index = img_index, explanation = gradCAM_expl)\n",
    "\n",
    "            conf_inc_pp_i = (calculate_conf_inc(model = model, img_index = img_index, explanation = gradCAM_pp_expl)).astype(int)\n",
    "            conf_inc_i = (calculate_conf_inc(model = model, img_index = img_index, explanation = gradCAM_expl)).astype(int)\n",
    "\n",
    "            win_pp_i, win_i = calculate_conf_win(model = model, img_index = img_index, explanation_pp = gradCAM_pp_expl, explanation = gradCAM_expl)\n",
    "\n",
    "            row_data = {'CID': CID, 'user': user, 'user id': user_id,'stimuli': stimulus_name\n",
    "                           , 'image index': img_index, 'true label': class_name_of_stimuli\n",
    "                           , 'human prediction': human_decision, 'human correct': human_correct\n",
    "                           , 'ai prediction': ai_decision\n",
    "                           , 'wasserstein distance ++': wasserstein_distance_GradCAM_pp\n",
    "                           , 'pearson correlation ++': pearson_corr_GradCAM_pp\n",
    "                           , 'mae ++': MAE_GradCAM_pp\n",
    "                           , 'confidence drop ++': conf_drop_pp_i, 'confidence increase ++': conf_inc_pp_i\n",
    "                           , 'win ++': (win_pp_i).astype(int)\n",
    "                           , 'wasserstein distance': wasserstein_distance_GradCAM\n",
    "                           , 'pearson correlation': pearson_corr_GradCAM\n",
    "                           , 'mae': MAE_GradCAM\n",
    "                           , 'confidence drop': conf_drop_i, 'confidence increase': conf_inc_i\n",
    "                           , 'win': (win_i).astype(int)}\n",
    "            \n",
    "            rows.append(row_data)\n",
    "            \n",
    "            CID += 1     \n",
    "    \n",
    "df = pd.DataFrame(rows, columns=columns)\n",
    "df['ai correct'] = (df['true label'] == df['ai prediction']).astype(int)\n",
    "merged_df = pd.merge(df, umfrage, on='user id', how='left')\n",
    "\n",
    "merged_df = pd.merge(merged_df, merged_df.groupby('stimuli')['human correct'].mean().reset_index(), on = 'stimuli')\n",
    "merged_df = merged_df.rename(columns={'human correct_y': 'average human image accuracy'})\n",
    "merged_df = merged_df.rename(columns={'human correct_x': 'human correct'})\n",
    "\n",
    "merged_df['both german cities visited'] = (merged_df['Berlin'] & merged_df['Hamburg'])\n",
    "merged_df['both israel cities visited'] = (merged_df['Tel Aviv'] & merged_df['Jerusalem'])\n",
    "\n",
    "merged_df = pd.merge(merged_df, merged_df.groupby('user')['human correct'].mean().reset_index(), on = 'user')\n",
    "merged_df = merged_df.rename(columns={'human correct_y': 'human accuracy'})\n",
    "merged_df = merged_df.rename(columns={'human correct_x': 'human correct'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filepath_GradCAM = 'XXX'\n",
    "filepath_Human_Heatmaps = 'XXX'\n",
    "filepath_StudyTimelines = 'XXX'\n",
    "filepath_Umfrage = 'XXX'\n",
    "\n",
    "heatmap_name_pp_1 = 'img_31.77692456977874,35.20849815507335CLASS_Jerusalem11_GradCAMpp'\n",
    "heatmap_name_1 = 'img_31.77692456977874,35.20849815507335CLASS_Jerusalem11_GradCAM'\n",
    "heatmap_name_pp_2 = 'img_32.0574547334093,34.765539699605604CLASS_Tel-Aviv7_GradCAMpp'\n",
    "heatmap_name_2 = 'img_32.0574547334093,34.765539699605604CLASS_Tel-Aviv7_GradCAM'\n",
    "heatmap_name_pp_3 = 'img_52.49465572860476,13.420295431164686CLASS_Berlin10_GradCAMpp'\n",
    "heatmap_name_3 = 'img_52.49465572860476,13.420295431164686CLASS_Berlin10_GradCAM'\n",
    "heatmap_name_pp_4 = 'img_53.548836682781165,9.981510481779921CLASS_Hamburg24_GradCAMpp'\n",
    "heatmap_name_4 = 'img_53.548836682781165,9.981510481779921CLASS_Hamburg24_GradCAM'\n",
    "\n",
    "img_index_1 = image_names_expl.index('img_31.77692456977874,35.20849815507335CLASS_Jerusalem11.png')\n",
    "img_index_2 = image_names_expl.index('img_32.0574547334093,34.765539699605604CLASS_Tel-Aviv7.png')\n",
    "img_index_3 = image_names_expl.index('img_52.49465572860476,13.420295431164686CLASS_Berlin10.png')\n",
    "img_index_4 = image_names_expl.index('img_53.548836682781165,9.981510481779921CLASS_Hamburg24.png')\n",
    "\n",
    "label_map = {0: 'B', 1: 'H', 2: 'T', 3: 'J'}\n",
    "\n",
    "#ai_decision = pd.Series(np.argmax(model.predict(np.expand_dims(X_expl[img_index], axis=0), verbose = 0)))\n",
    "#ai_decision = ai_decision.map(label_map).values[0]\n",
    "#print('AI pred: ', ai_decision)\n",
    "            \n",
    "gradCAM_pp_attention_1 = np.load(filepath_GradCAM+'/'+heatmap_name_pp_1+'.npy') \n",
    "gradCAM_attention_1 = np.load(filepath_GradCAM+'/'+heatmap_name_1+'.npy')\n",
    "human_attention_1 = np.load(filepath_Human_Heatmaps+'/User ID 2/'+'img_31.77692456977874,35.20849815507335CLASS_Jerusalem11'+'.npy') \n",
    "\n",
    "gradCAM_pp_attention_2 = np.load(filepath_GradCAM+'/'+heatmap_name_pp_2+'.npy') \n",
    "gradCAM_attention_2 = np.load(filepath_GradCAM+'/'+heatmap_name_2+'.npy')\n",
    "human_attention_2 = np.load(filepath_Human_Heatmaps+'/User ID 6/'+'img_32.0574547334093,34.765539699605604CLASS_Tel-Aviv7'+'.npy') \n",
    "\n",
    "gradCAM_pp_attention_3 = np.load(filepath_GradCAM+'/'+heatmap_name_pp_3+'.npy') \n",
    "gradCAM_attention_3 = np.load(filepath_GradCAM+'/'+heatmap_name_3+'.npy')\n",
    "human_attention_3 = np.load(filepath_Human_Heatmaps+'/User ID 6/'+'img_52.49465572860476,13.420295431164686CLASS_Berlin10'+'.npy') \n",
    "\n",
    "gradCAM_pp_attention_4 = np.load(filepath_GradCAM+'/'+heatmap_name_pp_4+'.npy') \n",
    "gradCAM_attention_4 = np.load(filepath_GradCAM+'/'+heatmap_name_4+'.npy')\n",
    "human_attention_4 = np.load(filepath_Human_Heatmaps+'/User ID 32/'+'img_53.548836682781165,9.981510481779921CLASS_Hamburg24'+'.npy') \n",
    "     \n",
    "gradCAM_pp_up_1 = upsample_gradCAM(gradCAM_pp_attention_1)\n",
    "gradCAM_up_1 = upsample_gradCAM(gradCAM_attention_1)\n",
    "gradCAM_pp_up_2 = upsample_gradCAM(gradCAM_pp_attention_2)\n",
    "gradCAM_up_2 = upsample_gradCAM(gradCAM_attention_2)\n",
    "gradCAM_pp_up_3 = upsample_gradCAM(gradCAM_pp_attention_3)\n",
    "gradCAM_up_3 = upsample_gradCAM(gradCAM_attention_3)\n",
    "gradCAM_pp_up_4 = upsample_gradCAM(gradCAM_pp_attention_4)\n",
    "gradCAM_up_4 = upsample_gradCAM(gradCAM_attention_4)\n",
    "\n",
    "original_img_1 = np.clip(X_expl[img_index_1], 0, 255).astype(\"uint8\")\n",
    "original_img_1 = cv2.cvtColor(original_img_1, cv2.COLOR_BGR2RGB)\n",
    "gradCAMpp_1 = visualize_gradCAM(gradCAM_pp_attention_1, img_index_1, alpha = 0.4)\n",
    "gradCAM_1 = visualize_gradCAM(gradCAM_attention_1, img_index_1, alpha = 0.4)\n",
    "human_attention_1 = visualize_human_attention(human_attention_1, img_index_1, alpha = 0.4)\n",
    "\n",
    "original_img_2 = np.clip(X_expl[img_index_2], 0, 255).astype(\"uint8\")\n",
    "original_img_2 = cv2.cvtColor(original_img_2, cv2.COLOR_BGR2RGB)\n",
    "gradCAMpp_2 = visualize_gradCAM(gradCAM_pp_attention_2, img_index_2, alpha = 0.4)\n",
    "gradCAM_2 = visualize_gradCAM(gradCAM_attention_2, img_index_2, alpha = 0.4)\n",
    "human_attention_2 = visualize_human_attention(human_attention_2, img_index_2, alpha = 0.4)\n",
    "\n",
    "original_img_3 = np.clip(X_expl[img_index_3], 0, 255).astype(\"uint8\")\n",
    "original_img_3 = cv2.cvtColor(original_img_3, cv2.COLOR_BGR2RGB)\n",
    "gradCAMpp_3 = visualize_gradCAM(gradCAM_pp_attention_3, img_index_3, alpha = 0.4)\n",
    "gradCAM_3 = visualize_gradCAM(gradCAM_attention_3, img_index_3, alpha = 0.4)\n",
    "human_attention_3 = visualize_human_attention(human_attention_3, img_index_3, alpha = 0.4)\n",
    "\n",
    "original_img_4 = np.clip(X_expl[img_index_4], 0, 255).astype(\"uint8\")\n",
    "original_img_4 = cv2.cvtColor(original_img_4, cv2.COLOR_BGR2RGB)\n",
    "gradCAMpp_4 = visualize_gradCAM(gradCAM_pp_attention_4, img_index_4, alpha = 0.4)\n",
    "gradCAM_4 = visualize_gradCAM(gradCAM_attention_4, img_index_4, alpha = 0.4)\n",
    "human_attention_4 = visualize_human_attention(human_attention_4, img_index_4, alpha = 0.4)\n",
    "\n",
    "fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
    "\n",
    "axes[0, 0].imshow(original_img_1)\n",
    "axes[0, 0].set_title('original image (Jerusalem)')\n",
    "\n",
    "axes[1, 0].imshow(gradCAMpp_1)\n",
    "axes[1, 0].set_title('Grad-CAM++ attention map')\n",
    "\n",
    "axes[2, 0].imshow(gradCAM_1)\n",
    "axes[2, 0].set_title('Grad-CAM attention map')\n",
    "\n",
    "axes[3, 0].imshow(human_attention_1)\n",
    "axes[3, 0].set_title('human attention map')\n",
    "\n",
    "axes[0, 1].imshow(original_img_2)\n",
    "axes[0, 1].set_title('original image (Tel Aviv)')\n",
    "\n",
    "axes[1, 1].imshow(gradCAMpp_2)\n",
    "axes[1, 1].set_title('Grad-CAM++ attention map')\n",
    "\n",
    "axes[2, 1].imshow(gradCAM_2)\n",
    "axes[2, 1].set_title('Grad-CAM attention map')\n",
    "\n",
    "axes[3, 1].imshow(human_attention_2)\n",
    "axes[3, 1].set_title('human attention map')\n",
    "\n",
    "axes[0, 2].imshow(original_img_3)\n",
    "axes[0, 2].set_title('original image (Berlin)')\n",
    "\n",
    "axes[1, 2].imshow(gradCAMpp_3)\n",
    "axes[1, 2].set_title('Grad-CAM++ attention map')\n",
    "\n",
    "axes[2, 2].imshow(gradCAM_3)\n",
    "axes[2, 2].set_title('Grad-CAM attention map')\n",
    "\n",
    "axes[3, 2].imshow(human_attention_3)\n",
    "axes[3, 2].set_title('human attention map')\n",
    "\n",
    "axes[0, 3].imshow(original_img_4)\n",
    "axes[0, 3].set_title('original image (Hamburg)')\n",
    "\n",
    "axes[1, 3].imshow(gradCAMpp_4)\n",
    "axes[1, 3].set_title('Grad-CAM++ attention map')\n",
    "\n",
    "axes[2, 3].imshow(gradCAM_4)\n",
    "axes[2, 3].set_title('Grad-CAM attention map')\n",
    "\n",
    "axes[3, 3].imshow(human_attention_4)\n",
    "axes[3, 3].set_title('human attention map')\n",
    "\n",
    "for ax_row in axes:\n",
    "    for ax in ax_row:\n",
    "        ax.axis('off')\n",
    "        \n",
    "plt.suptitle('Examples of AI and human attention maps for each class', fontsize=13)   \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('attentionmaps.png', dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_GradCAM = 'XXX'\n",
    "filepath_Human_Heatmaps = 'XXX'\n",
    "filepath_StudyTimelines = 'XXX'\n",
    "filepath_Umfrage = 'XXX'\n",
    "\n",
    "heatmap_name_pp_1 = 'img_31.768322995172117,35.18716699870462CLASS_Jerusalem21_GradCAMpp'\n",
    "heatmap_name_1 = 'img_31.768322995172117,35.18716699870462CLASS_Jerusalem21_GradCAM'\n",
    "heatmap_name_pp_2 = 'img_32.056770212808196,34.7732904745884CLASS_Tel-Aviv25_GradCAMpp'\n",
    "heatmap_name_2 = 'img_32.056770212808196,34.7732904745884CLASS_Tel-Aviv25_GradCAM'\n",
    "heatmap_name_pp_3 = 'img_52.50301222133445,13.328699938075516CLASS_Berlin16_GradCAMpp'\n",
    "heatmap_name_3 = 'img_52.50301222133445,13.328699938075516CLASS_Berlin16_GradCAM'\n",
    "heatmap_name_pp_4 = 'img_53.59310855742811,10.002262467095225CLASS_Hamburg20_GradCAMpp'\n",
    "heatmap_name_4 = 'img_53.59310855742811,10.002262467095225CLASS_Hamburg20_GradCAM'\n",
    "\n",
    "img_index_1 = image_names_expl.index('img_31.768322995172117,35.18716699870462CLASS_Jerusalem21.png')\n",
    "img_index_2 = image_names_expl.index('img_32.056770212808196,34.7732904745884CLASS_Tel-Aviv25.png')\n",
    "img_index_3 = image_names_expl.index('img_52.50301222133445,13.328699938075516CLASS_Berlin16.png')\n",
    "img_index_4 = image_names_expl.index('img_53.59310855742811,10.002262467095225CLASS_Hamburg20.png')\n",
    "\n",
    "label_map = {0: 'B', 1: 'H', 2: 'T', 3: 'J'}\n",
    "\n",
    "#ai_decision = pd.Series(np.argmax(model.predict(np.expand_dims(X_expl[img_index], axis=0), verbose = 0)))\n",
    "#ai_decision = ai_decision.map(label_map).values[0]\n",
    "#print('AI pred: ', ai_decision)\n",
    "            \n",
    "gradCAM_pp_attention_1 = np.load(filepath_GradCAM+'/'+heatmap_name_pp_1+'.npy') \n",
    "gradCAM_attention_1 = np.load(filepath_GradCAM+'/'+heatmap_name_1+'.npy')\n",
    "human_attention_1 = np.load(filepath_Human_Heatmaps+'/User ID 32/'+'img_31.768322995172117,35.18716699870462CLASS_Jerusalem21'+'.npy') \n",
    "\n",
    "gradCAM_pp_attention_2 = np.load(filepath_GradCAM+'/'+heatmap_name_pp_2+'.npy') \n",
    "gradCAM_attention_2 = np.load(filepath_GradCAM+'/'+heatmap_name_2+'.npy')\n",
    "human_attention_2 = np.load(filepath_Human_Heatmaps+'/User ID 19/'+'img_32.056770212808196,34.7732904745884CLASS_Tel-Aviv25'+'.npy') \n",
    "\n",
    "gradCAM_pp_attention_3 = np.load(filepath_GradCAM+'/'+heatmap_name_pp_3+'.npy') \n",
    "gradCAM_attention_3 = np.load(filepath_GradCAM+'/'+heatmap_name_3+'.npy')\n",
    "human_attention_3 = np.load(filepath_Human_Heatmaps+'/User ID 31/'+'img_52.50301222133445,13.328699938075516CLASS_Berlin16'+'.npy') \n",
    "\n",
    "gradCAM_pp_attention_4 = np.load(filepath_GradCAM+'/'+heatmap_name_pp_4+'.npy') \n",
    "gradCAM_attention_4 = np.load(filepath_GradCAM+'/'+heatmap_name_4+'.npy')\n",
    "human_attention_4 = np.load(filepath_Human_Heatmaps+'/User ID 41/'+'img_53.59310855742811,10.002262467095225CLASS_Hamburg20'+'.npy') \n",
    "     \n",
    "gradCAM_pp_up_1 = upsample_gradCAM(gradCAM_pp_attention_1)\n",
    "gradCAM_up_1 = upsample_gradCAM(gradCAM_attention_1)\n",
    "gradCAM_pp_up_2 = upsample_gradCAM(gradCAM_pp_attention_2)\n",
    "gradCAM_up_2 = upsample_gradCAM(gradCAM_attention_2)\n",
    "gradCAM_pp_up_3 = upsample_gradCAM(gradCAM_pp_attention_3)\n",
    "gradCAM_up_3 = upsample_gradCAM(gradCAM_attention_3)\n",
    "gradCAM_pp_up_4 = upsample_gradCAM(gradCAM_pp_attention_4)\n",
    "gradCAM_up_4 = upsample_gradCAM(gradCAM_attention_4)\n",
    "\n",
    "original_img_1 = np.clip(X_expl[img_index_1], 0, 255).astype(\"uint8\")\n",
    "original_img_1 = cv2.cvtColor(original_img_1, cv2.COLOR_BGR2RGB)\n",
    "gradCAMpp_1 = visualize_gradCAM(gradCAM_pp_attention_1, img_index_1, alpha = 0.4)\n",
    "gradCAM_1 = visualize_gradCAM(gradCAM_attention_1, img_index_1, alpha = 0.4)\n",
    "human_attention_1 = visualize_human_attention(human_attention_1, img_index_1, alpha = 0.4)\n",
    "\n",
    "original_img_2 = np.clip(X_expl[img_index_2], 0, 255).astype(\"uint8\")\n",
    "original_img_2 = cv2.cvtColor(original_img_2, cv2.COLOR_BGR2RGB)\n",
    "gradCAMpp_2 = visualize_gradCAM(gradCAM_pp_attention_2, img_index_2, alpha = 0.4)\n",
    "gradCAM_2 = visualize_gradCAM(gradCAM_attention_2, img_index_2, alpha = 0.4)\n",
    "human_attention_2 = visualize_human_attention(human_attention_2, img_index_2, alpha = 0.4)\n",
    "\n",
    "original_img_3 = np.clip(X_expl[img_index_3], 0, 255).astype(\"uint8\")\n",
    "original_img_3 = cv2.cvtColor(original_img_3, cv2.COLOR_BGR2RGB)\n",
    "gradCAMpp_3 = visualize_gradCAM(gradCAM_pp_attention_3, img_index_3, alpha = 0.4)\n",
    "gradCAM_3 = visualize_gradCAM(gradCAM_attention_3, img_index_3, alpha = 0.4)\n",
    "human_attention_3 = visualize_human_attention(human_attention_3, img_index_3, alpha = 0.4)\n",
    "\n",
    "original_img_4 = np.clip(X_expl[img_index_4], 0, 255).astype(\"uint8\")\n",
    "original_img_4 = cv2.cvtColor(original_img_4, cv2.COLOR_BGR2RGB)\n",
    "gradCAMpp_4 = visualize_gradCAM(gradCAM_pp_attention_4, img_index_4, alpha = 0.4)\n",
    "gradCAM_4 = visualize_gradCAM(gradCAM_attention_4, img_index_4, alpha = 0.4)\n",
    "human_attention_4 = visualize_human_attention(human_attention_4, img_index_4, alpha = 0.4)\n",
    "\n",
    "fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
    "\n",
    "axes[0, 0].imshow(original_img_1)\n",
    "axes[0, 0].set_title('original Image (Jerusalem)')\n",
    "\n",
    "axes[1, 0].imshow(gradCAMpp_1)\n",
    "axes[1, 0].set_title('Grad-CAM++ attention map')\n",
    "\n",
    "axes[2, 0].imshow(gradCAM_1)\n",
    "axes[2, 0].set_title('Grad-CAM attention map')\n",
    "\n",
    "axes[3, 0].imshow(human_attention_1)\n",
    "axes[3, 0].set_title('human attention map')\n",
    "\n",
    "axes[0, 1].imshow(original_img_2)\n",
    "axes[0, 1].set_title('original Image (Tel Aviv)')\n",
    "\n",
    "axes[1, 1].imshow(gradCAMpp_2)\n",
    "axes[1, 1].set_title('Grad-CAM++ attention map')\n",
    "\n",
    "axes[2, 1].imshow(gradCAM_2)\n",
    "axes[2, 1].set_title('Grad-CAM attention map')\n",
    "\n",
    "axes[3, 1].imshow(human_attention_2)\n",
    "axes[3, 1].set_title('human attention map')\n",
    "\n",
    "axes[0, 2].imshow(original_img_3)\n",
    "axes[0, 2].set_title('original Image (Berlin)')\n",
    "\n",
    "axes[1, 2].imshow(gradCAMpp_3)\n",
    "axes[1, 2].set_title('Grad-CAM++ attention map')\n",
    "\n",
    "axes[2, 2].imshow(gradCAM_3)\n",
    "axes[2, 2].set_title('Grad-CAM attention map')\n",
    "\n",
    "axes[3, 2].imshow(human_attention_3)\n",
    "axes[3, 2].set_title('human attention map')\n",
    "\n",
    "axes[0, 3].imshow(original_img_4)\n",
    "axes[0, 3].set_title('original Image (Hamburg)')\n",
    "\n",
    "axes[1, 3].imshow(gradCAMpp_4)\n",
    "axes[1, 3].set_title('Grad-CAM++ attention map')\n",
    "\n",
    "axes[2, 3].imshow(gradCAM_4)\n",
    "axes[2, 3].set_title('Grad-CAM attention map')\n",
    "\n",
    "axes[3, 3].imshow(human_attention_4)\n",
    "axes[3, 3].set_title('human attention map')\n",
    "\n",
    "for ax_row in axes:\n",
    "    for ax in ax_row:\n",
    "        ax.axis('off')\n",
    "        \n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('attentionmaps2.png', dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_GradCAM = 'XXX'\n",
    "filepath_Human_Heatmaps = 'XXX'\n",
    "filepath_StudyTimelines = 'XXX'\n",
    "filepath_Umfrage = 'XXX'\n",
    "\n",
    "heatmap_name_pp_1 = 'img_31.763214851900877,35.20885600974014CLASS_Jerusalem17_GradCAMpp'\n",
    "heatmap_name_1 = 'img_31.763214851900877,35.20885600974014CLASS_Jerusalem17_GradCAM'\n",
    "heatmap_name_pp_2 = 'img_32.0840529613404,34.78256479352893CLASS_Tel-Aviv10_GradCAMpp'\n",
    "heatmap_name_2 = 'img_32.0840529613404,34.78256479352893CLASS_Tel-Aviv10_GradCAM'\n",
    "heatmap_name_pp_3 = 'img_52.52473202450015,13.348054821127073CLASS_Berlin22_GradCAMpp'\n",
    "heatmap_name_3 = 'img_52.52473202450015,13.348054821127073CLASS_Berlin22_GradCAM'\n",
    "heatmap_name_pp_4 = 'img_53.548502941071426,10.003652538092444CLASS_Hamburg23_GradCAMpp'\n",
    "heatmap_name_4 = 'img_53.548502941071426,10.003652538092444CLASS_Hamburg23_GradCAM'\n",
    "\n",
    "img_index_1 = image_names_expl.index('img_31.763214851900877,35.20885600974014CLASS_Jerusalem17.png')\n",
    "img_index_2 = image_names_expl.index('img_32.0840529613404,34.78256479352893CLASS_Tel-Aviv10.png')\n",
    "img_index_3 = image_names_expl.index('img_52.52473202450015,13.348054821127073CLASS_Berlin22.png')\n",
    "img_index_4 = image_names_expl.index('img_53.548502941071426,10.003652538092444CLASS_Hamburg23.png')\n",
    "\n",
    "label_map = {0: 'B', 1: 'H', 2: 'T', 3: 'J'}\n",
    "\n",
    "#ai_decision = pd.Series(np.argmax(model.predict(np.expand_dims(X_expl[img_index], axis=0), verbose = 0)))\n",
    "#ai_decision = ai_decision.map(label_map).values[0]\n",
    "#print('AI pred: ', ai_decision)\n",
    "            \n",
    "gradCAM_pp_attention_1 = np.load(filepath_GradCAM+'/'+heatmap_name_pp_1+'.npy') \n",
    "gradCAM_attention_1 = np.load(filepath_GradCAM+'/'+heatmap_name_1+'.npy')\n",
    "human_attention_1 = np.load(filepath_Human_Heatmaps+'/User ID 28/'+'img_31.763214851900877,35.20885600974014CLASS_Jerusalem17'+'.npy') \n",
    "\n",
    "gradCAM_pp_attention_2 = np.load(filepath_GradCAM+'/'+heatmap_name_pp_2+'.npy') \n",
    "gradCAM_attention_2 = np.load(filepath_GradCAM+'/'+heatmap_name_2+'.npy')\n",
    "human_attention_2 = np.load(filepath_Human_Heatmaps+'/User ID 21/'+'img_32.0840529613404,34.78256479352893CLASS_Tel-Aviv10'+'.npy') \n",
    "\n",
    "gradCAM_pp_attention_3 = np.load(filepath_GradCAM+'/'+heatmap_name_pp_3+'.npy') \n",
    "gradCAM_attention_3 = np.load(filepath_GradCAM+'/'+heatmap_name_3+'.npy')\n",
    "human_attention_3 = np.load(filepath_Human_Heatmaps+'/User ID 26/'+'img_52.52473202450015,13.348054821127073CLASS_Berlin22'+'.npy') \n",
    "\n",
    "gradCAM_pp_attention_4 = np.load(filepath_GradCAM+'/'+heatmap_name_pp_4+'.npy') \n",
    "gradCAM_attention_4 = np.load(filepath_GradCAM+'/'+heatmap_name_4+'.npy')\n",
    "human_attention_4 = np.load(filepath_Human_Heatmaps+'/User ID 44/'+'img_53.548502941071426,10.003652538092444CLASS_Hamburg23'+'.npy') \n",
    "     \n",
    "gradCAM_pp_up_1 = upsample_gradCAM(gradCAM_pp_attention_1)\n",
    "gradCAM_up_1 = upsample_gradCAM(gradCAM_attention_1)\n",
    "gradCAM_pp_up_2 = upsample_gradCAM(gradCAM_pp_attention_2)\n",
    "gradCAM_up_2 = upsample_gradCAM(gradCAM_attention_2)\n",
    "gradCAM_pp_up_3 = upsample_gradCAM(gradCAM_pp_attention_3)\n",
    "gradCAM_up_3 = upsample_gradCAM(gradCAM_attention_3)\n",
    "gradCAM_pp_up_4 = upsample_gradCAM(gradCAM_pp_attention_4)\n",
    "gradCAM_up_4 = upsample_gradCAM(gradCAM_attention_4)\n",
    "\n",
    "original_img_1 = np.clip(X_expl[img_index_1], 0, 255).astype(\"uint8\")\n",
    "original_img_1 = cv2.cvtColor(original_img_1, cv2.COLOR_BGR2RGB)\n",
    "gradCAMpp_1 = visualize_gradCAM(gradCAM_pp_attention_1, img_index_1, alpha = 0.4)\n",
    "gradCAM_1 = visualize_gradCAM(gradCAM_attention_1, img_index_1, alpha = 0.4)\n",
    "human_attention_1 = visualize_human_attention(human_attention_1, img_index_1, alpha = 0.4)\n",
    "\n",
    "original_img_2 = np.clip(X_expl[img_index_2], 0, 255).astype(\"uint8\")\n",
    "original_img_2 = cv2.cvtColor(original_img_2, cv2.COLOR_BGR2RGB)\n",
    "gradCAMpp_2 = visualize_gradCAM(gradCAM_pp_attention_2, img_index_2, alpha = 0.4)\n",
    "gradCAM_2 = visualize_gradCAM(gradCAM_attention_2, img_index_2, alpha = 0.4)\n",
    "human_attention_2 = visualize_human_attention(human_attention_2, img_index_2, alpha = 0.4)\n",
    "\n",
    "original_img_3 = np.clip(X_expl[img_index_3], 0, 255).astype(\"uint8\")\n",
    "original_img_3 = cv2.cvtColor(original_img_3, cv2.COLOR_BGR2RGB)\n",
    "gradCAMpp_3 = visualize_gradCAM(gradCAM_pp_attention_3, img_index_3, alpha = 0.4)\n",
    "gradCAM_3 = visualize_gradCAM(gradCAM_attention_3, img_index_3, alpha = 0.4)\n",
    "human_attention_3 = visualize_human_attention(human_attention_3, img_index_3, alpha = 0.4)\n",
    "\n",
    "original_img_4 = np.clip(X_expl[img_index_4], 0, 255).astype(\"uint8\")\n",
    "original_img_4 = cv2.cvtColor(original_img_4, cv2.COLOR_BGR2RGB)\n",
    "gradCAMpp_4 = visualize_gradCAM(gradCAM_pp_attention_4, img_index_4, alpha = 0.4)\n",
    "gradCAM_4 = visualize_gradCAM(gradCAM_attention_4, img_index_4, alpha = 0.4)\n",
    "human_attention_4 = visualize_human_attention(human_attention_4, img_index_4, alpha = 0.4)\n",
    "\n",
    "fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
    "\n",
    "axes[0, 0].imshow(original_img_1)\n",
    "axes[0, 0].set_title('original Image (Jerusalem)')\n",
    "\n",
    "axes[1, 0].imshow(gradCAMpp_1)\n",
    "axes[1, 0].set_title('Grad-CAM++ attention map')\n",
    "\n",
    "axes[2, 0].imshow(gradCAM_1)\n",
    "axes[2, 0].set_title('Grad-CAM attention map')\n",
    "\n",
    "axes[3, 0].imshow(human_attention_1)\n",
    "axes[3, 0].set_title('human attention map')\n",
    "\n",
    "axes[0, 1].imshow(original_img_2)\n",
    "axes[0, 1].set_title('original Image (Tel Aviv)')\n",
    "\n",
    "axes[1, 1].imshow(gradCAMpp_2)\n",
    "axes[1, 1].set_title('Grad-CAM++ attention map')\n",
    "\n",
    "axes[2, 1].imshow(gradCAM_2)\n",
    "axes[2, 1].set_title('Grad-CAM attention map')\n",
    "\n",
    "axes[3, 1].imshow(human_attention_2)\n",
    "axes[3, 1].set_title('human attention map')\n",
    "\n",
    "axes[0, 2].imshow(original_img_3)\n",
    "axes[0, 2].set_title('original Image (Berlin)')\n",
    "\n",
    "axes[1, 2].imshow(gradCAMpp_3)\n",
    "axes[1, 2].set_title('Grad-CAM++ attention map')\n",
    "\n",
    "axes[2, 2].imshow(gradCAM_3)\n",
    "axes[2, 2].set_title('Grad-CAM attention map')\n",
    "\n",
    "axes[3, 2].imshow(human_attention_3)\n",
    "axes[3, 2].set_title('human attention map')\n",
    "\n",
    "axes[0, 3].imshow(original_img_4)\n",
    "axes[0, 3].set_title('original Image (Hamburg)')\n",
    "\n",
    "axes[1, 3].imshow(gradCAMpp_4)\n",
    "axes[1, 3].set_title('Grad-CAM++ attention map')\n",
    "\n",
    "axes[2, 3].imshow(gradCAM_4)\n",
    "axes[2, 3].set_title('Grad-CAM attention map')\n",
    "\n",
    "axes[3, 3].imshow(human_attention_4)\n",
    "axes[3, 3].set_title('human attention map')\n",
    "\n",
    "for ax_row in axes:\n",
    "    for ax in ax_row:\n",
    "        ax.axis('off')\n",
    "        \n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('attentionmaps3.png', dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_GradCAM = 'XXX'\n",
    "filepath_Human_Heatmaps = 'XXX'\n",
    "filepath_StudyTimelines = 'XXX'\n",
    "filepath_Umfrage = 'XXX'\n",
    "\n",
    "heatmap_name_pp_1 = 'img_31.77692456977874,35.20849815507335CLASS_Jerusalem11_GradCAMpp'\n",
    "heatmap_name_1 = 'img_31.77692456977874,35.20849815507335CLASS_Jerusalem11_GradCAM'\n",
    "heatmap_name_pp_2 = 'img_32.0574547334093,34.765539699605604CLASS_Tel-Aviv7_GradCAMpp'\n",
    "heatmap_name_2 = 'img_32.0574547334093,34.765539699605604CLASS_Tel-Aviv7_GradCAM'\n",
    "heatmap_name_pp_3 = 'img_52.49465572860476,13.420295431164686CLASS_Berlin10_GradCAMpp'\n",
    "heatmap_name_3 = 'img_52.49465572860476,13.420295431164686CLASS_Berlin10_GradCAM'\n",
    "heatmap_name_pp_4 = 'img_53.548836682781165,9.981510481779921CLASS_Hamburg24_GradCAMpp'\n",
    "heatmap_name_4 = 'img_53.548836682781165,9.981510481779921CLASS_Hamburg24_GradCAM'\n",
    "\n",
    "img_index_1 = image_names_expl.index('img_31.77692456977874,35.20849815507335CLASS_Jerusalem11.png')\n",
    "img_index_2 = image_names_expl.index('img_32.0574547334093,34.765539699605604CLASS_Tel-Aviv7.png')\n",
    "img_index_3 = image_names_expl.index('img_52.49465572860476,13.420295431164686CLASS_Berlin10.png')\n",
    "img_index_4 = image_names_expl.index('img_53.548836682781165,9.981510481779921CLASS_Hamburg24.png')\n",
    "\n",
    "label_map = {0: 'B', 1: 'H', 2: 'T', 3: 'J'}\n",
    "\n",
    "#ai_decision = pd.Series(np.argmax(model.predict(np.expand_dims(X_expl[img_index], axis=0), verbose = 0)))\n",
    "#ai_decision = ai_decision.map(label_map).values[0]\n",
    "#print('AI pred: ', ai_decision)\n",
    "            \n",
    "gradCAM_pp_attention_1 = np.load(filepath_GradCAM+'/'+heatmap_name_pp_1+'.npy') \n",
    "gradCAM_attention_1 = np.load(filepath_GradCAM+'/'+heatmap_name_1+'.npy')\n",
    "\n",
    "gradCAM_pp_attention_2 = np.load(filepath_GradCAM+'/'+heatmap_name_pp_2+'.npy') \n",
    "gradCAM_attention_2 = np.load(filepath_GradCAM+'/'+heatmap_name_2+'.npy')\n",
    "\n",
    "\n",
    "gradCAM_pp_attention_3 = np.load(filepath_GradCAM+'/'+heatmap_name_pp_3+'.npy') \n",
    "gradCAM_attention_3 = np.load(filepath_GradCAM+'/'+heatmap_name_3+'.npy')\n",
    "\n",
    "\n",
    "gradCAM_pp_attention_4 = np.load(filepath_GradCAM+'/'+heatmap_name_pp_4+'.npy') \n",
    "gradCAM_attention_4 = np.load(filepath_GradCAM+'/'+heatmap_name_4+'.npy')\n",
    "\n",
    "     \n",
    "gradCAM_pp_expl_1 = explain_gradCAM(activations = gradCAM_pp_attention_1, img_index = img_index_1)\n",
    "gradCAM_expl_1 = explain_gradCAM(activations = gradCAM_attention_1, img_index = img_index_1)\n",
    "\n",
    "gradCAM_pp_expl_2 = explain_gradCAM(activations = gradCAM_pp_attention_2, img_index = img_index_2)\n",
    "gradCAM_expl_2 = explain_gradCAM(activations = gradCAM_attention_2, img_index = img_index_2)\n",
    "\n",
    "gradCAM_pp_expl_3 = explain_gradCAM(activations = gradCAM_pp_attention_3, img_index = img_index_3)\n",
    "gradCAM_expl_3 = explain_gradCAM(activations = gradCAM_attention_3, img_index = img_index_3)\n",
    "\n",
    "gradCAM_pp_expl_4 = explain_gradCAM(activations = gradCAM_pp_attention_4, img_index = img_index_4)\n",
    "gradCAM_expl_4 = explain_gradCAM(activations = gradCAM_attention_4, img_index = img_index_4)\n",
    "\n",
    "original_img_1 = np.clip(X_expl[img_index_1], 0, 255).astype(\"uint8\")\n",
    "original_img_1 = cv2.cvtColor(original_img_1, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "original_img_2 = np.clip(X_expl[img_index_2], 0, 255).astype(\"uint8\")\n",
    "original_img_2 = cv2.cvtColor(original_img_2, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "original_img_3 = np.clip(X_expl[img_index_3], 0, 255).astype(\"uint8\")\n",
    "original_img_3 = cv2.cvtColor(original_img_3, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "original_img_4 = np.clip(X_expl[img_index_4], 0, 255).astype(\"uint8\")\n",
    "original_img_4 = cv2.cvtColor(original_img_4, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "fig, axes = plt.subplots(3, 4, figsize=(12, 9))\n",
    "\n",
    "axes[0, 0].imshow(original_img_1)\n",
    "axes[0, 0].set_title('original image (Jerusalem)')\n",
    "\n",
    "axes[1, 0].imshow(gradCAM_pp_expl_1)\n",
    "axes[1, 0].set_title('Grad-CAM++ explanation map')\n",
    "\n",
    "axes[2, 0].imshow(gradCAM_expl_1)\n",
    "axes[2, 0].set_title('Grad-CAM explanation map')\n",
    "\n",
    "axes[0, 1].imshow(original_img_2)\n",
    "axes[0, 1].set_title('original image (Tel Aviv)')\n",
    "\n",
    "axes[1, 1].imshow(gradCAM_pp_expl_2)\n",
    "axes[1, 1].set_title('Grad-CAM++ explanation map')\n",
    "\n",
    "axes[2, 1].imshow(gradCAM_expl_2)\n",
    "axes[2, 1].set_title('Grad-CAM explanation map')\n",
    "\n",
    "axes[0, 2].imshow(original_img_3)\n",
    "axes[0, 2].set_title('original image (Berlin)')\n",
    "\n",
    "axes[1, 2].imshow(gradCAM_pp_expl_3)\n",
    "axes[1, 2].set_title('Grad-CAM++ explanation map')\n",
    "\n",
    "axes[2, 2].imshow(gradCAM_expl_3)\n",
    "axes[2, 2].set_title('Grad-CAM explanation map')\n",
    "\n",
    "axes[0, 3].imshow(original_img_4)\n",
    "axes[0, 3].set_title('original image (Hamburg)')\n",
    "\n",
    "axes[1, 3].imshow(gradCAM_pp_expl_4)\n",
    "axes[1, 3].set_title('Grad-CAM++ explanation map')\n",
    "\n",
    "axes[2, 3].imshow(gradCAM_expl_4)\n",
    "axes[2, 3].set_title('Grad-CAM explanation map')\n",
    "\n",
    "for ax_row in axes:\n",
    "    for ax in ax_row:\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.suptitle('Examples of AI explanation maps for each class', fontsize=13)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('explanationmaps.png', dpi=300)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conf_drop_pp = 0\n",
    "conf_drop = 0\n",
    "\n",
    "conf_inc_pp = 0\n",
    "conf_inc = 0\n",
    "\n",
    "win_pp = 0\n",
    "win = 0\n",
    "\n",
    "for i in range(X_expl.shape[0]):\n",
    "    \n",
    "    gradCAM_pp_activations = gradCAMplusplus(img_array = np.expand_dims(X_expl[i], axis=0), model = model, grad_conv_model = grad_model, classifier_layer_names= classifier_layer_names)\n",
    "    gradCAM_activations = gradCAM(img_array = np.expand_dims(X_expl[i], axis=0), model = model, grad_conv_model = grad_model, classifier_layer_names= classifier_layer_names)\n",
    "    \n",
    "    print('************************************')\n",
    "    print('Grad-CAM ++ Input for evaluation:')\n",
    "    gradCAM_pp_expl = explain_gradCAM(activations = gradCAM_pp_activations, img_index = i)\n",
    "    print('Grad-CAM Input for evaluation:')\n",
    "    gradCAM_expl = explain_gradCAM(activations = gradCAM_activations, img_index = i)\n",
    "    \n",
    "    conf_drop_pp_i = calculate_conf_drop(model = model, img_index = i, explanation = gradCAM_pp_expl)\n",
    "    conf_drop_pp += conf_drop_pp_i\n",
    "\n",
    "    conf_drop_i = calculate_conf_drop(model = model, img_index = i, explanation = gradCAM_expl)\n",
    "    conf_drop += conf_drop_i\n",
    "    \n",
    "    conf_inc_pp_i = calculate_conf_inc(model = model, img_index = i, explanation = gradCAM_pp_expl)\n",
    "    conf_inc_pp += conf_inc_pp_i\n",
    "\n",
    "    conf_inc_i = calculate_conf_inc(model = model, img_index = i, explanation = gradCAM_expl)\n",
    "    conf_inc += conf_inc_i\n",
    "    \n",
    "    win_pp_i, win_i = calculate_conf_win(model = model, img_index = i, explanation_pp = gradCAM_pp_expl, explanation = gradCAM_expl)\n",
    "    win_pp += win_pp_i\n",
    "    win += win_i\n",
    "    \n",
    "avg_conf_drop_pp = conf_drop_pp*100/(X_expl.shape[0])\n",
    "avg_conf_drop = conf_drop*100/(X_expl.shape[0])\n",
    "\n",
    "avg_conf_inc_pp = conf_inc_pp/(X_expl.shape[0])*100\n",
    "avg_conf_inc = conf_inc/(X_expl.shape[0])*100\n",
    "\n",
    "avg_win_pp = win_pp/(X_expl.shape[0])*100\n",
    "avg_win = win/(X_expl.shape[0])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('average Confidence drop % (lower is better)')\n",
    "print('Grad-CAM++', round(avg_conf_drop_pp, 2))\n",
    "print('Grad-CAM', round(avg_conf_drop,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Confidence inc % (higher is better)')\n",
    "print('Grad-CAM++',avg_conf_inc_pp)\n",
    "print('Grad-CAM',avg_conf_inc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Win % (higher is better)')\n",
    "print('Grad-CAM++',round(avg_win_pp,2))\n",
    "print('Grad-CAM',round(avg_win,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_drop_pp = 0\n",
    "conf_drop = 0\n",
    "\n",
    "conf_inc_pp = 0\n",
    "conf_inc = 0\n",
    "\n",
    "win_pp = 0\n",
    "win = 0\n",
    "X_expl = X_test\n",
    "y_int_expl = y_int_test\n",
    "print(X_expl.shape)\n",
    "for i in range(X_expl.shape[0]):\n",
    "    print(i)\n",
    "    gradCAM_pp_activations = gradCAMplusplus(img_array = np.expand_dims(X_expl[i], axis=0), model = model, grad_conv_model = grad_model, classifier_layer_names= classifier_layer_names)\n",
    "    gradCAM_activations = gradCAM(img_array = np.expand_dims(X_expl[i], axis=0), model = model, grad_conv_model = grad_model, classifier_layer_names= classifier_layer_names)\n",
    "    \n",
    "    print('************************************')\n",
    "    print('Grad-CAM ++ Input for evaluation:')\n",
    "    gradCAM_pp_expl = explain_gradCAM(activations = gradCAM_pp_activations, img_index = i)\n",
    "    print('Grad-CAM Input for evaluation:')\n",
    "    gradCAM_expl = explain_gradCAM(activations = gradCAM_activations, img_index = i)\n",
    "    \n",
    "    conf_drop_pp_i = calculate_conf_drop(model = model, img_index = i, explanation = gradCAM_pp_expl)\n",
    "    conf_drop_pp += conf_drop_pp_i\n",
    "\n",
    "    conf_drop_i = calculate_conf_drop(model = model, img_index = i, explanation = gradCAM_expl)\n",
    "    conf_drop += conf_drop_i\n",
    "    \n",
    "    conf_inc_pp_i = calculate_conf_inc(model = model, img_index = i, explanation = gradCAM_pp_expl)\n",
    "    conf_inc_pp += conf_inc_pp_i\n",
    "\n",
    "    conf_inc_i = calculate_conf_inc(model = model, img_index = i, explanation = gradCAM_expl)\n",
    "    conf_inc += conf_inc_i\n",
    "    \n",
    "    win_pp_i, win_i = calculate_conf_win(model = model, img_index = i, explanation_pp = gradCAM_pp_expl, explanation = gradCAM_expl)\n",
    "    win_pp += win_pp_i\n",
    "    win += win_i\n",
    "    \n",
    "avg_conf_drop_pp = conf_drop_pp*100/(X_expl.shape[0])\n",
    "avg_conf_drop = conf_drop*100/(X_expl.shape[0])\n",
    "\n",
    "avg_conf_inc_pp = conf_inc_pp/(X_expl.shape[0])*100\n",
    "avg_conf_inc = conf_inc/(X_expl.shape[0])*100\n",
    "\n",
    "avg_win_pp = win_pp/(X_expl.shape[0])*100\n",
    "avg_win = win/(X_expl.shape[0])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('average Confidence drop % (lower is better)')\n",
    "print('Grad-CAM++', round(avg_conf_drop_pp, 2))\n",
    "print('Grad-CAM', round(avg_conf_drop,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Confidence inc % (higher is better)')\n",
    "print('Grad-CAM++',round(avg_conf_inc_pp,2))\n",
    "print('Grad-CAM',round(avg_conf_inc,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Win % (higher is better)')\n",
    "print('Grad-CAM++',round(avg_win_pp,2))\n",
    "print('Grad-CAM',round(avg_win,2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare Heatmaps with Silhouttenkoeffizient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get human heatmaps for image\n",
    "def extract_heatmaps_for_image(image_id, user_folders_path):\n",
    "    image_heatmaps = {}\n",
    "    user_folders = os.listdir(user_folders_path)\n",
    "    for user_folder in user_folders:\n",
    "        heatmap_path = os.path.join(user_folders_path, user_folder, f\"{image_id}.npy\")\n",
    "        if os.path.exists(heatmap_path):\n",
    "            heatmap = np.load(heatmap_path)\n",
    "            image_heatmaps[user_folder] = heatmap\n",
    "            \n",
    "    return image_heatmaps\n",
    "\n",
    "#calculate wasserstein distance for one pair of heatmaps\n",
    "def calculate_distance(heatmap1, heatmap2):\n",
    "    heatmap1 = heatmap1.flatten()\n",
    "    heatmap2 = heatmap2.flatten()\n",
    "    similarity = wasserstein_distance(heatmap1, heatmap2)\n",
    "    return similarity\n",
    "\n",
    "#calculate a_j (average wasserstein distance of human heatmap to all other human heatmaps)\n",
    "def average_distance_to_others(heatmap, all_heatmaps):\n",
    "    distances = []\n",
    "    for other_heatmap in all_heatmaps:\n",
    "        if np.array_equal(heatmap, other_heatmap):\n",
    "            continue \n",
    "        distance = calculate_distance(heatmap, other_heatmap)\n",
    "        distances.append(distance)\n",
    "    average_distance = np.mean(distances)\n",
    "    return average_distance\n",
    "\n",
    "#calculate b_j (wasserstein distance of human heatmap to XAI heatmap)\n",
    "def distance_to_XAI(heatmap, xai_heatmaps):\n",
    "    distances = []\n",
    "    for other_heatmap in xai_heatmaps:\n",
    "        distance = calculate_distance(heatmap, other_heatmap)\n",
    "        distances.append(distance)\n",
    "    return distances\n",
    "\n",
    "def distance_to_others(heatmap, all_heatmaps):\n",
    "    distances = []\n",
    "    for other_heatmap in all_heatmaps:\n",
    "        if np.array_equal(heatmap, other_heatmap):\n",
    "            continue \n",
    "        distance = calculate_distance(heatmap, other_heatmap)\n",
    "        distances.append(distance)\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_folders_path = 'XXX'\n",
    "filepath_GradCAM = 'XXX'\n",
    "all_files = os.listdir(filepath_GradCAM)\n",
    "filtered_files = [file for file in all_files if file.endswith(\"pp.npy\")]\n",
    "filtered_files = [file.split('_GradCAMpp')[0] for file in filtered_files]\n",
    "\n",
    "distribution_of_silhoutten_pp = []\n",
    "distribution_of_silhoutten = []\n",
    "i = 0\n",
    "for stimuli in filtered_files:\n",
    "    i += 1\n",
    "    print(i, \": Calculating for image:\", stimuli)\n",
    "    image_id = stimuli\n",
    "    #get GradCAM and GradCAM++ explanations\n",
    "    gradCAM_pp_attention = np.load(filepath_GradCAM+'/'+image_id+'_GradCAMpp.npy')\n",
    "    gradCAM_attention = np.load(filepath_GradCAM+'/'+image_id+'_GradCAM.npy')\n",
    "    gradCAM_pp_up = upsample_gradCAM(gradCAM_pp_attention)\n",
    "    gradCAM_up = upsample_gradCAM(gradCAM_attention)\n",
    "    xai_heatmaps = {}\n",
    "    xai_heatmaps[\"gradCAM_pp\"] = gradCAM_pp_up\n",
    "    xai_heatmaps[\"gradCAM\"] = gradCAM_up\n",
    "\n",
    "    image_heatmaps = extract_heatmaps_for_image(image_id, user_folders_path)\n",
    "    \n",
    "    silhoutten_koeffizient_GradCAM = []\n",
    "    silhoutten_koeffizient_GradCAM_pp = []\n",
    "    \n",
    "    for user_id, heatmap in image_heatmaps.items():\n",
    "        average_distance_human = average_distance_to_others(heatmap, list(image_heatmaps.values()))\n",
    "        #print(f\"Average similarity of heatmap for {user_id} to all others: {average_distance_human}\")\n",
    "        distance_human_to_XAI = distance_to_XAI(heatmap, list(xai_heatmaps.values()))\n",
    "        #print(f\"Similarity of heatmap for {user_id} to xai: {distance_human_to_XAI}\")\n",
    "        silhoutten_koeffizient_GradCAM_pp.append((distance_human_to_XAI[0]-average_distance_human)/max(distance_human_to_XAI[0],average_distance_human)) \n",
    "        silhoutten_koeffizient_GradCAM.append((distance_human_to_XAI[1]-average_distance_human)/max(distance_human_to_XAI[1],average_distance_human))\n",
    "    \n",
    "    distribution_of_silhoutten_pp.append(np.mean(silhoutten_koeffizient_GradCAM_pp))\n",
    "    distribution_of_silhoutten.append(np.mean(silhoutten_koeffizient_GradCAM))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_folders_path = 'XXX'\n",
    "filepath_GradCAM = 'XXX'\n",
    "all_files = os.listdir(filepath_GradCAM)\n",
    "filtered_files = [file for file in all_files if file.endswith(\"pp.npy\")]\n",
    "filtered_files = [file.split('_GradCAMpp')[0] for file in filtered_files]\n",
    "i = 0\n",
    "distribution_of_distances = []\n",
    "\n",
    "for stimuli in filtered_files:\n",
    "    i += 1\n",
    "    print(i, \": Calculating for image:\", stimuli)\n",
    "    image_id = stimuli\n",
    "    image_heatmaps = extract_heatmaps_for_image(image_id, user_folders_path)\n",
    "    \n",
    "    for user_id, heatmap in image_heatmaps.items():\n",
    "        distribution_of_distances.append(distance_to_others(heatmap, list(image_heatmaps.values()))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('XAITrackingX11.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_human_XAIpp = np.mean(data['wasserstein distance ++'])\n",
    "std_human_XAIpp = np.std(data['wasserstein distance ++'])\n",
    "\n",
    "mean_human_XAI = np.mean(data['wasserstein distance'])\n",
    "std_human_XAI = np.std(data['wasserstein distance'])\n",
    "\n",
    "dist_of_inter_human_distances = np.array(list(itertools.chain.from_iterable(distribution_of_distances)))\n",
    "mean_inter_human = np.mean(dist_of_inter_human_distances)\n",
    "std_inter_human = np.std(dist_of_inter_human_distances)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "sns.set_theme(style=\"white\")\n",
    "sns.histplot(data['wasserstein distance ++'], label='AI-human (Grad-CAM++)', color='blue', kde=True)\n",
    "sns.histplot(data['wasserstein distance'], label='AI-human (Grad-CAM)', color='orange', kde=True)\n",
    "#sns.histplot(np.array([item for sublist in distribution_of_distances for item in sublist]), label='inter-human', color='red', kde=True)\n",
    "sns.histplot(dist_of_inter_human_distances, label='inter-human', color='red', kde=True)\n",
    "plt.fill_between([mean_human_XAIpp - std_human_XAIpp, mean_human_XAIpp + std_human_XAIpp], 0, 1300, color='blue', alpha=0.1, label='±1 STDEV (AI-human for Grad-CAM++)')\n",
    "plt.fill_between([mean_human_XAI - std_human_XAI, mean_human_XAI + std_human_XAI], 0, 1300, color='orange', alpha=0.1, label='±1 STDEV (AI-human for Grad-CAM)')\n",
    "plt.fill_between([mean_inter_human - std_inter_human, mean_inter_human + std_inter_human], 0, 1300, color='red', alpha=0.1, label='±1 STDEV (inter-human)')\n",
    "\n",
    "plt.axvline(mean_human_XAIpp, color='blue', linestyle='--', label='average (AI-human for Grad-CAM++)')\n",
    "plt.axvline(mean_human_XAI, color='orange', linestyle='--', label='average (AI-human for Grad-CAM)')\n",
    "plt.axvline(mean_inter_human, color='red', linestyle='--', label='average (inter-human)')\n",
    "\n",
    "plt.xlabel(\"Earth mover's distance\")\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlim(0,np.max(data['wasserstein distance ++']))\n",
    "plt.ylim(0,1300)\n",
    "plt.legend()\n",
    "plt.title(\"Distribution of inter-human and AI-human Earth mover's distance\")\n",
    "\n",
    "plt.savefig('wasserstein_distance_interhuman_vs_humanXAI_vs_humanXAI.png', dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(data['wasserstein distance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "res_XAI_pp = stats.shapiro(data['wasserstein distance ++'])\n",
    "res_XAI = stats.shapiro(data['wasserstein distance'])\n",
    "res_human = stats.shapiro(dist_of_inter_human_distances)\n",
    "print(res_XAI_pp)\n",
    "print(res_XAI)\n",
    "print(res_human)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = stats.levene(data['wasserstein distance ++'], dist_of_inter_human_distances)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistic, p_value = mannwhitneyu(data['wasserstein distance ++'], dist_of_inter_human_distances, alternative = 'greater')\n",
    "\n",
    "print('********************')\n",
    "print('Mann Whitney U test:')\n",
    "print('********************')\n",
    "print('is a nonparametric test of the null hypothesis that, for randomly selected values X and Y from two populations, the probability of X being greater than Y is equal to the probability of Y being greater than X. The Mann-Whitney U test is a nonparametric test of the null hypothesis that the distribution underlying sample x is the same as the distribution underlying sample y. It is often used as a test of difference in location between distributions.')\n",
    "print()\n",
    "print('p-value:', p_value)\n",
    "print(statistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistic, p_value = mannwhitneyu(data['wasserstein distance'], dist_of_inter_human_distances, alternative = 'greater')\n",
    "\n",
    "print('********************')\n",
    "print('Mann Whitney U test:')\n",
    "print('********************')\n",
    "print('is a nonparametric test of the null hypothesis that, for randomly selected values X and Y from two populations, the probability of X being greater than Y is equal to the probability of Y being greater than X. The Mann-Whitney U test is a nonparametric test of the null hypothesis that the distribution underlying sample x is the same as the distribution underlying sample y. It is often used as a test of difference in location between distributions.')\n",
    "print()\n",
    "print('p-value:', p_value)\n",
    "print(statistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistic, p_value = ttest_ind(data['wasserstein distance ++'], dist_of_inter_human_distances, alternative = 'greater')\n",
    "\n",
    "print('********************')\n",
    "print('two sample t test:')\n",
    "print('********************')\n",
    "print('This is a test for the null hypothesis that 2 independent samples have identical average (expected) values. This test assumes that the populations have identical variances by default.')\n",
    "print()\n",
    "print('p-value:', p_value)\n",
    "print(statistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"white\")\n",
    "mean_GradCAM_pp = np.mean(distribution_of_silhoutten_pp)\n",
    "std_GradCAM_pp = np.std(distribution_of_silhoutten_pp)\n",
    "\n",
    "mean_GradCAM = np.mean(distribution_of_silhoutten)\n",
    "std_GradCAM = np.std(distribution_of_silhoutten)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(distribution_of_silhoutten, label='Grad-CAM', color='orange', kde=True, binwidth=0.04)\n",
    "sns.histplot(distribution_of_silhoutten_pp, label='Grad-CAM++', color='blue', kde=True, binwidth=0.04)\n",
    "\n",
    "plt.fill_between([mean_GradCAM_pp - std_GradCAM_pp, mean_GradCAM_pp + std_GradCAM_pp], 0, 50, color='blue', alpha=0.1, label='±1 STDEV (Grad-CAM++)')\n",
    "plt.fill_between([mean_GradCAM - std_GradCAM, mean_GradCAM + std_GradCAM], 0, 50, color='orange', alpha=0.1, label='±1 STDEV (Grad-CAM)')\n",
    "plt.xlabel('average silhouette metric')\n",
    "plt.axvline(mean_GradCAM, color='orange', linestyle='dashed', linewidth=1, label='average (Grad-CAM)')\n",
    "plt.axvline(mean_GradCAM_pp, color='blue', linestyle='dashed', linewidth=1, label='average (Grad-CAM++)')\n",
    "plt.axvline(0, color='black', linestyle='dashed', linewidth=1, label='no clustering structure')\n",
    "#plt.ylabel('Frequency')\n",
    "plt.xlim(-0.1,1)\n",
    "plt.ylim(0,45)\n",
    "#plt.ylim(0,150)\n",
    "plt.legend(loc = \"upper left\")\n",
    "plt.title('Distribution of silhouette metric')\n",
    "plt.plot()\n",
    "plt.savefig('silhouette_metric.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_pp = stats.shapiro(distribution_of_silhoutten_pp)\n",
    "res = stats.shapiro(distribution_of_silhoutten)\n",
    "print(res_pp)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistic, p_value = mannwhitneyu(distribution_of_silhoutten_pp, 0, alternative = 'greater')\n",
    "\n",
    "print('********************')\n",
    "print('Mann Whitney U test:')\n",
    "print('********************')\n",
    "print('is a nonparametric test of the null hypothesis that, for randomly selected values X and Y from two populations, the probability of X being greater than Y is equal to the probability of Y being greater than X. The Mann-Whitney U test is a nonparametric test of the null hypothesis that the distribution underlying sample x is the same as the distribution underlying sample y. It is often used as a test of difference in location between distributions.')\n",
    "print()\n",
    "print('p-value:', p_value)\n",
    "print(statistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistic, p_value = mannwhitneyu(distribution_of_silhoutten, 0, alternative = 'greater')\n",
    "\n",
    "print('********************')\n",
    "print('Mann Whitney U test:')\n",
    "print('********************')\n",
    "print('is a nonparametric test of the null hypothesis that, for randomly selected values X and Y from two populations, the probability of X being greater than Y is equal to the probability of Y being greater than X. The Mann-Whitney U test is a nonparametric test of the null hypothesis that the distribution underlying sample x is the same as the distribution underlying sample y. It is often used as a test of difference in location between distributions.')\n",
    "print()\n",
    "print('p-value:', p_value)\n",
    "print(statistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(distribution_of_silhoutten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(distribution_of_silhoutten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(distribution_of_silhoutten)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
